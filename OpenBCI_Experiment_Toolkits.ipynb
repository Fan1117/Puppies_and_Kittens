{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# OpenBCI Experiment Toolkits\n",
    "This is a tutorial about how to create external triggers in a video, assemble external trigger receiver with photoresistors, label, segment and analyze neural data with OpenBCI hardwares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Start\n",
    "1. Install opencv packages by ```pip install opencv-python```\n",
    "2. Collect 5 images in each class (e.g. Puppies and Kiddens) and save them in ./images. Name them with the routime: class_number.jpg (e.g. Cat_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a customized experiment video with black-and-white square flicker embedded.\n",
    "\n",
    "#### Quick Start\n",
    "1. Install OpenCV packages by ```pip install opencv-python```\n",
    "2. Find the folder ```./Images/Image_Class/Cat```. Put the kitten images in this folder.\n",
    "3. Find the folder ```./Images/Image_Class/Dog```. Put the puppie images in this folder.\n",
    "4. In the terminal, change directory to the current folder or type ```cd /Puppies_and_Kittens/```. Then type ```python ExternalTriggerCreator.py -l settings.json```. The experiment video ```project_video.mp4``` and the corresponding labels ```label.txt``` will be output in the folder ```./```\n",
    "\n",
    "#### Arguments in json file with default values\n",
    "1. \"image_base_path\": \"./Images/\", directory that stores all the images including welcome page and classification image pages.\n",
    "2. \"image_types\": [\"Cat\", \"Dog\"], \n",
    "3. \"video_time\": 3000, default time for each image to appear (ms)\n",
    "4. \"trigger_interval\": 100, time for the trigger to flick one time (ms)\n",
    "5. \"flick_times\": 5, times for the trigger to flick at the begining and ending of the video (ms)\n",
    "6. \"fps\": 20, \n",
    "7. \"screen_size\": [500, 400], \n",
    "8. \"time_range_per_image\": [3.5, 6.5], \n",
    "9. \"video_output\": \"project_video.mp4\",\n",
    "10. \"label_output\": \"labels.txt\",\n",
    "11. \"trigger_position\": [0, 300, 100, 400], [x_start, y_start, x_end, y_end]\n",
    "\n",
    "#### Experiment Video\n",
    "![Experiment Video](./ExperimentImage.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Assemble an external trigger receiver with photoresistor and OpenBCI hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:  Collect neural data with OpenBCI hardware and GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Label, segment and analyze the neural data.\n",
    "\n",
    "On windows, the collected data can be found in ```C:\\Users\\username\\Documents\\OpenBCI_GUI\\Recordings```. We will label this data set with the text file ```labels.txt``` created in **Step 1**.\n",
    "\n",
    "#### First of all, let's read the collected neural data with columns: 'index', 'ch1', ... , 'ch8', 'A7', 'TimeStamp'.  \n",
    "***'index'***: index of data row  \n",
    "***'ch1' to 'ch8'***: EEG channel data  \n",
    "***'A7'***: analog channel data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "openbci_datapath = '../record.txt'\n",
    "sample_rate = 250\n",
    "\n",
    "data = pd.read_csv(openbci_datapath, sep=\",\", header=6, index_col=False, names=['index', 'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8', 'A5', 'A6', 'A7', 'Time', 'TimeStamp'], usecols=['ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8', 'A7', 'TimeStamp'])\n",
    "print(\"OpenBCI data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Timestamps = (data['TimeStamp'].to_numpy() - data['TimeStamp'].to_numpy()[0])/1000\n",
    "print(\"Timestamps shape: \", Timestamps.shape)\n",
    "print(\"Total length of data: \", Timestamps[-1], 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select EEG data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "EEG_data = data[['ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8']].to_numpy()\n",
    "print(\"EEG data shape: \", EEG_data.shape)\n",
    "for i in range(EEG_data.shape[1]):\n",
    "    plt.plot(EEG_data[:,i]/np.mean(EEG_data[:,i])+i)\n",
    "plt.xticks(ticks=np.arange(0, round(Timestamps[-1]+1)*sample_rate, round(Timestamps[-1]+1)*sample_rate/4), \n",
    "           labels=np.arange(0, round(Timestamps[-1]+1),round(Timestamps[-1]+1)/4))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select analog data and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "analog_data = data['A7']\n",
    "print(\"analog data shape: \", analog_data.shape)\n",
    "plt.plot(analog_data)\n",
    "plt.xticks(ticks=np.arange(0, round(Timestamps[-1]+1)*sample_rate, round(Timestamps[-1]+1)*sample_rate/5), \n",
    "           labels=np.arange(0, round(Timestamps[-1]+1),round(Timestamps[-1]+1)/5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect spikes in analog channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "peaks, _ = find_peaks(analog_data, height=(5, 20), distance=0.1*250) # distance is essential!\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(analog_data)\n",
    "ax[0].plot(peaks, analog_data[peaks], \"x\")\n",
    "ax[0].set_ylim(-2, 20)\n",
    "ax[0].title.set_text('Summary')\n",
    "ax[0].set_xticks(ticks=np.arange(0, len(analog_data), len(analog_data)/4))\n",
    "ax[0].set_xticklabels(labels=np.arange(0, len(analog_data)/250,len(analog_data)/250/4))\n",
    "\n",
    "# Start \n",
    "ax[1].plot(analog_data)\n",
    "ax[1].plot(peaks, analog_data[peaks], \"x\")\n",
    "ax[1].set_xlim(1000,3000)\n",
    "ax[1].set_ylim(-2, 20)\n",
    "ax[1].title.set_text('4 - 12 s')\n",
    "ax[1].set_xticks(ticks=np.arange(1000, 3000, 2000/4))\n",
    "ax[1].set_xticklabels(labels=np.arange(1000/250, 3000/250, 2000/250/4))\n",
    "\n",
    "# End\n",
    "ax[2].plot(analog_data)\n",
    "ax[2].plot(peaks, analog_data[peaks], \"x\")\n",
    "ax[2].set_xlim(28000,30000)\n",
    "ax[2].set_ylim(-2, 20)\n",
    "ax[2].title.set_text('112 - 120 s')\n",
    "ax[2].set_xticks(ticks=np.arange(28000, 30000, 2000/4))\n",
    "ax[2].set_xticklabels(labels=np.arange(28000/250, 30000/250, 2000/250/4))\n",
    "\n",
    "fig.savefig('fig.png')\n",
    "\n",
    "print(\"Total number of peaks: \", len(peaks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the figure above, the combination of the first 4 peaks is the indicator of the start of the video. From Step 1, there will be totally 22 peaks indicating the start of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image_peaks = peaks[4:4+22]\n",
    "len(image_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert labels into the raw data set: Puppies: 1 vs Kittens: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "label_path = '../labels_version_3.txt'\n",
    "\n",
    "label_data = pd.read_csv(label_path, sep=',', index_col=False, names=['label_index', 'label', 'filename'])\n",
    "index_array = label_data['label_index'].to_numpy()\n",
    "labels = np.zeros(len(EEG_data))\n",
    "\n",
    "for i in range(len(image_peaks)):\n",
    "    peak_timestamp = image_peaks[i]\n",
    "    labels[peak_timestamp] = index_array[i]+1\n",
    "    \n",
    "\n",
    "for i in range(EEG_data.shape[1]):\n",
    "    plt.plot(EEG_data[:,i]/np.mean(EEG_data[:,i])-i)\n",
    "plt.xticks(ticks=np.arange(0, round(Timestamps[-1]+1)*sample_rate, round(Timestamps[-1]+1)*sample_rate/4), \n",
    "           labels=np.arange(0, round(Timestamps[-1]+1),round(Timestamps[-1]+1)/4))\n",
    "plt.plot(labels-i-2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.tight_layout\n",
    "plt.legend(['ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7', 'ch8', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
